{
  "date": "2026-02-13",
  "score_avg": 31.87,
  "primary_dist": {
    "infra": 7,
    "models": 8
  },
  "top_entities": [],
  "briefing": {
    "signals": [],
    "risks": [],
    "watch": [],
    "entities_top": []
  },
  "items": [
    {
      "title": "TSMC vs Intel Foundry vs Samsung Foundry 2026",
      "link": "https://semiwiki.com/semiconductor-manufacturers/tsmc/366523-tsmc-vs-intel-foundry-vs-samsung-foundry-2026/",
      "published": "2026-02-13T14:00:18+00:00",
      "summary": "<p>The global semiconductor industry sits at the foundation of modern technology, powering everything from smartphones and cloud data centers to artificial intelligence, automobiles, and national defense systems. At the center of advanced chip manufacturing are three major players: <a href=\"https://semiwiki.com/category/semiconductor-manufacturers/tsmc/\"><strong>TSMC</strong></a>, <strong>Samsung Foundry</strong>, and <a href=\"https://semiwiki.com/category/semiconductor-manufacturers/intel/\"><strong>Intel Foundry</strong></a>&#8230; <a class=\"read-more\" href=\"https://semiwiki.com/semiconductor-manufacturers/tsmc/366523-tsmc-vs-intel-foundry-vs-samsung-foundry-2026/\">Read More </a></p>\n<p>The post <a href=\"https://semiwiki.com/semiconductor-manufacturers/tsmc/366523-tsmc-vs-intel-foundry-vs-samsung-foundry-2026/\">TSMC vs Intel Foundry vs Samsung Foundry 2026</a> appeared first on <a href=\"https://semiwiki.com\">SemiWiki</a>.</p>",
      "source": "SemiWiki",
      "feed_tags": [
        "infra",
        "chips"
      ],
      "score": 63,
      "primary": "infra",
      "tags": [
        "infra"
      ],
      "_rid": 1,
      "url": "https://semiwiki.com/semiconductor-manufacturers/tsmc/366523-tsmc-vs-intel-foundry-vs-samsung-foundry-2026/",
      "entities": [],
      "why": "<p>The global semiconductor industry sits at the foundation of modern technology, powering everything from smartphones and cloud data centers to artificial inte"
    },
    {
      "title": "As AI Grows More Complex, Model Builders Rely on NVIDIA",
      "link": "https://blogs.nvidia.com/blog/leading-models-nvidia/",
      "published": "2025-12-11T19:19:57+00:00",
      "summary": "Unveiling what it describes as the most capable model series yet for professional knowledge work, OpenAI launched GPT-5.2 in December. The model was trained and deployed on NVIDIA infrastructure, including NVIDIA Hopper and GB200 NVL72 systems. GPT-5.3 Codex — the first OpenAI agentic coding model to help build itself — was released in February and\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/leading-models-nvidia/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
      "source": "NVIDIA Blog (AI)",
      "feed_tags": [
        "infra",
        "chips"
      ],
      "score": 47,
      "primary": "infra",
      "tags": [
        "infra",
        "models"
      ],
      "_rid": 2,
      "url": "https://blogs.nvidia.com/blog/leading-models-nvidia/",
      "entities": [],
      "why": "Unveiling what it describes as the most capable model series yet for professional knowledge work, OpenAI launched GPT-5.2 in December. The model was trained and"
    },
    {
      "title": "NVIDIA Research Shapes Physical AI",
      "link": "https://blogs.nvidia.com/blog/physical-ai-research-siggraph-2025/",
      "published": "2025-08-11T15:00:38+00:00",
      "summary": "AI and graphics research breakthroughs in neural rendering, 3D generation and world simulation power robotics, autonomous vehicles and content creation.",
      "source": "NVIDIA Blog (AI)",
      "feed_tags": [
        "infra",
        "chips"
      ],
      "score": 36,
      "primary": "infra",
      "tags": [
        "infra",
        "models"
      ],
      "_rid": 3,
      "url": "https://blogs.nvidia.com/blog/physical-ai-research-siggraph-2025/",
      "entities": [],
      "why": "AI and graphics research breakthroughs in neural rendering, 3D generation and world simulation power robotics, autonomous vehicles and content creation."
    },
    {
      "title": "Automated Optimization Modeling via a Localizable Error-Driven Perspective",
      "link": "https://arxiv.org/abs/2602.11164",
      "published": "2026-02-13T05:00:00+00:00",
      "summary": "arXiv:2602.11164v1 Announce Type: new \nAbstract: Automated optimization modeling via Large Language Models (LLMs) has emerged as a promising approach to assist complex human decision-making. While post-training has become a pivotal technique to enhance LLMs' capabilities in this domain, its effectiveness is severely constrained by the scarcity and underutilization of high-quality training data. However, through a detailed profiling of error patterns across various problem-response pairs drawn from post-training, we identify two fundamental limitations of existing automated optimization modeling approaches: (L1) the sparsity of error-specific problems and (L2) the sparse rewards associated with difficult problems. We demonstrate that these limitations can result in suboptimal performance in domain-specific post-training for LLMs. To tackle the above two limitations, we propose a novel error-driven learning framework -- namely, auto\\textbf{m}ated opt\\textbf{i}mization modeli\\textbf{n}g via a localizable error-\\textbf{d}riven perspective (MIND) -- that customizes the whole model training framework from data synthesis to post-training. MIND is based on our key observation of the unique localizable patterns in error propagation of optimization modelings, that is, modeling errors may remain localized to specific semantic segments and do not propagate throughout the entire solution. Thus, in contrast to holistic reasoning tasks such as mathematical proofs, MIND leverages the construction of a focused, high-density training corpus and proposes \\textbf{D}ynamic Supervised \\textbf{F}ine-Tuning \\textbf{P}olicy \\textbf{O}ptimization (DFPO) to tackle difficult problems through localized refinement. Experiments on six benchmarks demonstrate that MIND consistently outperforms all the state-of-the-art automated optimization modeling approaches.",
      "source": "arXiv cs.LG",
      "feed_tags": [
        "papers",
        "models"
      ],
      "score": 35,
      "primary": "models",
      "tags": [
        "models"
      ],
      "_rid": 4,
      "url": "https://arxiv.org/abs/2602.11164",
      "entities": [],
      "why": "arXiv:2602.11164v1 Announce Type: new \nAbstract: Automated optimization modeling via Large Language Models (LLMs) has emerged as a promising approach to assist "
    },
    {
      "title": "Reaching Across the Isles: UK-LLM Brings AI to UK Languages With NVIDIA Nemotron",
      "link": "https://blogs.nvidia.com/blog/uk-llm-nemotron/",
      "published": "2025-09-14T01:00:21+00:00",
      "summary": "Celtic languages — including Cornish, Irish, Scottish Gaelic and Welsh — are the U.K.’s oldest living languages. To empower their speakers, the UK-LLM sovereign AI initiative is building an AI model based on NVIDIA Nemotron that can reason in both English and Welsh, a language spoken by about 850,000 people in Wales today. Enabling high-quality\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/uk-llm-nemotron/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
      "source": "NVIDIA Blog (AI)",
      "feed_tags": [
        "infra",
        "chips"
      ],
      "score": 33,
      "primary": "models",
      "tags": [
        "infra",
        "models",
        "geopol"
      ],
      "_rid": 5,
      "url": "https://blogs.nvidia.com/blog/uk-llm-nemotron/",
      "entities": [],
      "why": "Celtic languages — including Cornish, Irish, Scottish Gaelic and Welsh — are the U.K.’s oldest living languages. To empower their speakers, the UK-LLM sovereign"
    },
    {
      "title": "Latent Generative Solvers for Generalizable Long-Term Physics Simulation",
      "link": "https://arxiv.org/abs/2602.11229",
      "published": "2026-02-13T05:00:00+00:00",
      "summary": "arXiv:2602.11229v1 Announce Type: new \nAbstract: We study long-horizon surrogate simulation across heterogeneous PDE systems. We introduce Latent Generative Solvers (LGS), a two-stage framework that (i) maps diverse PDE states into a shared latent physics space with a pretrained VAE, and (ii) learns probabilistic latent dynamics with a Transformer trained by flow matching. Our key mechanism is an uncertainty knob that perturbs latent inputs during training and inference, teaching the solver to correct off-manifold rollout drift and stabilizing autoregressive prediction. We further use flow forcing to update a system descriptor (context) from model-generated trajectories, aligning train/test conditioning and improving long-term stability. We pretrain on a curated corpus of $\\sim$2.5M trajectories at $128^2$ resolution spanning 12 PDE families. LGS matches strong deterministic neural-operator baselines on short horizons while substantially reducing rollout drift on long horizons. Learning in latent space plus efficient architectural choices yields up to \\textbf{70$\\times$} lower FLOPs than non-generative baselines, enabling scalable pretraining. We also show efficient adaptation to an out-of-distribution $256^2$ Kolmogorov flow dataset under limited finetuning budgets. Overall, LGS provides a practical route toward generalizable, uncertainty-aware neural PDE solvers that are more reliable for long-term forecasting and downstream scientific workflows.",
      "source": "arXiv cs.AI",
      "feed_tags": [
        "papers",
        "models"
      ],
      "score": 32,
      "primary": "models",
      "tags": [
        "models"
      ],
      "_rid": 6,
      "url": "https://arxiv.org/abs/2602.11229",
      "entities": [],
      "why": "arXiv:2602.11229v1 Announce Type: new \nAbstract: We study long-horizon surrogate simulation across heterogeneous PDE systems. We introduce Latent Generative Sol"
    },
    {
      "title": "KBVQ-MoE: KLT-guided SVD with Bias-Corrected Vector Quantization for MoE Large Language Models",
      "link": "https://arxiv.org/abs/2602.11184",
      "published": "2026-02-13T05:00:00+00:00",
      "summary": "arXiv:2602.11184v1 Announce Type: new \nAbstract: Mixture of Experts (MoE) models have achieved great success by significantly improving performance while maintaining computational efficiency through sparse expert activation. However, their enormous parameter sizes and memory demands pose major challenges for deployment in resource-constrained environments. Vector Quantization (VQ) offers a promising approach for ultra-low-bit compression in Large Language Models (LLMs) by leveraging a codebook, where weight vectors are mapped to the most similar discrete codewords. Yet, directly applying VQ to MoEs often leads to substantial performance degradation due to two critical obstacles: (1) redundant representations among experts cause VQ to repeatedly quantize similar representations for each expert, resulting in inefficient use of limited codebook capacity; and (2) cumulative output bias is amplified by expert aggregation in MoE layers, leading to distributional shifts in the quantized outputs. To address these issues, we propose KBVQ-MoE, a novel VQ framework to enhance extremely low-bit quantization for MoE-based LLMs. KBVQ-MoE integrates two techniques: (1) input-driven redundancy elimination, where a Karhunen-Loeve Transform (KLT) guided singular value decomposition (SVD) extracts dominant weight components and shares them across experts; and (2) bias-corrected output stabilization, where vector quantization is applied only to expert-specific (non-redundant) representations and the quantized outputs are corrected via channel-wise affine compensation. Experiments on various MoE LLMs demonstrate that KBVQ-MoE preserves accuracy substantially better than existing quantization methods. For example, 3-bit quantization of Qwen1.5-MoE-A2.7B achieves an average accuracy of 67.99, nearly identical to the FP16 baseline of 68.07, underscoring KBVQ-MoE's potential for efficient deployment on edge devices and other resource-constrained platforms.",
      "source": "arXiv cs.LG",
      "feed_tags": [
        "papers",
        "models"
      ],
      "score": 32,
      "primary": "models",
      "tags": [
        "models"
      ],
      "_rid": 7,
      "url": "https://arxiv.org/abs/2602.11184",
      "entities": [],
      "why": "arXiv:2602.11184v1 Announce Type: new \nAbstract: Mixture of Experts (MoE) models have achieved great success by significantly improving performance while mainta"
    },
    {
      "title": "How Memory Technology Is Powering the Next Era of Compute",
      "link": "https://semiwiki.com/ip/rambus/366449-how-memory-technology-is-powering-the-next-era-of-compute/",
      "published": "2026-02-11T18:00:42+00:00",
      "summary": "<p>For more than a decade, progress in artificial intelligence has been framed almost entirely through the lens of compute. Faster GPUs, denser accelerators, and higher TOPS defined each new generation. But as generative and agentic AI enter their next phase, that framing is no longer sufficient. The most advanced AI systems today&#8230; <a class=\"read-more\" href=\"https://semiwiki.com/ip/rambus/366449-how-memory-technology-is-powering-the-next-era-of-compute/\">Read More </a></p>\n<p>The post <a href=\"https://semiwiki.com/ip/rambus/366449-how-memory-technology-is-powering-the-next-era-of-compute/\">How Memory Technology Is Powering the Next Era of Compute</a> appeared first on <a href=\"https://semiwiki.com\">SemiWiki</a>.</p>",
      "source": "SemiWiki",
      "feed_tags": [
        "infra",
        "chips"
      ],
      "score": 31,
      "primary": "infra",
      "tags": [
        "infra",
        "models"
      ],
      "_rid": 8,
      "url": "https://semiwiki.com/ip/rambus/366449-how-memory-technology-is-powering-the-next-era-of-compute/",
      "entities": [],
      "why": "<p>For more than a decade, progress in artificial intelligence has been framed almost entirely through the lens of compute. Faster GPUs, denser accelerators, an"
    },
    {
      "title": "Isambard-AI, the UK’s Most Powerful AI Supercomputer, Goes Live",
      "link": "https://blogs.nvidia.com/blog/isambard-ai/",
      "published": "2025-07-17T17:00:50+00:00",
      "summary": "The University of Bristol’s Isambard-AI, powered by NVIDIA Grace Hopper Superchips, delivers 21 exaflops of AI performance, making it the fastest system in the U.K. and among the most energy-efficient globally.",
      "source": "NVIDIA Blog (AI)",
      "feed_tags": [
        "infra",
        "chips"
      ],
      "score": 28,
      "primary": "infra",
      "tags": [
        "infra"
      ],
      "_rid": 9,
      "url": "https://blogs.nvidia.com/blog/isambard-ai/",
      "entities": [],
      "why": "The University of Bristol’s Isambard-AI, powered by NVIDIA Grace Hopper Superchips, delivers 21 exaflops of AI performance, making it the fastest system in the "
    },
    {
      "title": "Innovation to Impact: How NVIDIA Research Fuels Transformative Work in AI, Graphics and Beyond",
      "link": "https://blogs.nvidia.com/blog/nvidia-research-ai-graphics/",
      "published": "2025-03-20T00:00:24+00:00",
      "summary": "The roots of many of NVIDIA’s landmark innovations — the foundational technology that powers AI, accelerated computing, real-time ray tracing and seamlessly connected data centers — can be found in the company’s research organization, a global team of around 400 experts in fields including computer architecture, generative AI, graphics and robotics. Established in 2006 and\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/nvidia-research-ai-graphics/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
      "source": "NVIDIA Blog (AI)",
      "feed_tags": [
        "infra",
        "chips"
      ],
      "score": 28,
      "primary": "infra",
      "tags": [
        "infra"
      ],
      "_rid": 10,
      "url": "https://blogs.nvidia.com/blog/nvidia-research-ai-graphics/",
      "entities": [],
      "why": "The roots of many of NVIDIA’s landmark innovations — the foundational technology that powers AI, accelerated computing, real-time ray tracing and seamlessly con"
    },
    {
      "title": "Applications Now Open for $60,000 NVIDIA Graduate Fellowship Awards",
      "link": "https://blogs.nvidia.com/blog/applications-open-graduate-fellowship-awards-2025/",
      "published": "2025-08-13T15:00:02+00:00",
      "summary": "Bringing together the world’s brightest minds and the latest accelerated computing technology leads to powerful breakthroughs that help tackle some of the biggest research problems. To foster such innovation, the NVIDIA Graduate Fellowship Program provides grants, mentors and technical support to doctoral students doing outstanding research relevant to NVIDIA technologies. The program, in its 25th\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/applications-open-graduate-fellowship-awards-2025/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
      "source": "NVIDIA Blog (AI)",
      "feed_tags": [
        "infra",
        "chips"
      ],
      "score": 26,
      "primary": "infra",
      "tags": [
        "infra",
        "models"
      ],
      "_rid": 11,
      "url": "https://blogs.nvidia.com/blog/applications-open-graduate-fellowship-awards-2025/",
      "entities": [],
      "why": "Bringing together the world’s brightest minds and the latest accelerated computing technology leads to powerful breakthroughs that help tackle some of the bigge"
    },
    {
      "title": "NVIDIA Releases New AI Models and Developer Tools to Advance Autonomous Vehicle Ecosystem",
      "link": "https://blogs.nvidia.com/blog/autonomous-vehicle-ecosystem-ai-models-developer-tools/",
      "published": "2025-06-11T10:55:36+00:00",
      "summary": "Autonomous vehicle (AV) stacks are evolving from many distinct models to a unified, end-to-end architecture that executes driving actions directly from sensor data. This transition to using larger models is drastically increasing the demand for high-quality, physically based sensor data for training, testing and validation. To help accelerate the development of next-generation AV architectures, NVIDIA\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/autonomous-vehicle-ecosystem-ai-models-developer-tools/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
      "source": "NVIDIA Blog (AI)",
      "feed_tags": [
        "infra",
        "chips"
      ],
      "score": 26,
      "primary": "models",
      "tags": [
        "models"
      ],
      "_rid": 12,
      "url": "https://blogs.nvidia.com/blog/autonomous-vehicle-ecosystem-ai-models-developer-tools/",
      "entities": [],
      "why": "Autonomous vehicle (AV) stacks are evolving from many distinct models to a unified, end-to-end architecture that executes driving actions directly from sensor d"
    },
    {
      "title": "Giving AI Agents Access to a Compiled Design and Verification Database",
      "link": "https://semiwiki.com/eda/amiq-eda/366471-giving-ai-agents-access-to-a-compiled-design-and-verification-database/",
      "published": "2026-02-12T16:00:40+00:00",
      "summary": "<p>A few weeks ago, I had the chance to work with AMIQ EDA as they <a href=\"https://www.einpresswire.com/article/886585001/amiq-eda-gives-ai-agents-access-to-essential-design-and-verification-data\">introduced</a> a new product: DVT MCP Server. I was quite intrigued by the role it will play in AI-assisted chip design and verification, so I wanted to learn more. I spoke with Gabriel Busuioc, the AI Assistant team leader at AMIQ EDA, to understand more about the product and how&#8230; <a class=\"read-more\" href=\"https://semiwiki.com/eda/amiq-eda/366471-giving-ai-agents-access-to-a-compiled-design-and-verification-database/\">Read More </a></p>\n<p>The post <a href=\"https://semiwiki.com/eda/amiq-eda/366471-giving-ai-agents-access-to-a-compiled-design-and-verification-database/\">Giving AI Agents Access to a Compiled Design and Verification Database</a> appeared first on <a href=\"https://semiwiki.com\">SemiWiki</a>.</p>",
      "source": "SemiWiki",
      "feed_tags": [
        "infra",
        "chips"
      ],
      "score": 26,
      "primary": "models",
      "tags": [
        "models"
      ],
      "_rid": 13,
      "url": "https://semiwiki.com/eda/amiq-eda/366471-giving-ai-agents-access-to-a-compiled-design-and-verification-database/",
      "entities": [],
      "why": "<p>A few weeks ago, I had the chance to work with AMIQ EDA as they <a href=\"https://www.einpresswire.com/article/886585001/amiq-eda-gives-ai-agents-access-to-es"
    },
    {
      "title": "It’s a Sign: AI Platform for Teaching American Sign Language Aims to Bridge Communication Gaps",
      "link": "https://blogs.nvidia.com/blog/ai-sign-language/",
      "published": "2025-02-20T14:30:18+00:00",
      "summary": "American Sign Language is the third most prevalent language in the United States — but there are vastly fewer AI tools developed with ASL data than data representing the country’s most common languages, English and Spanish. NVIDIA, the American Society for Deaf Children and creative agency Hello Monday are helping close this gap with Signs,\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/ai-sign-language/\">\n\t\tRead Article\t\t<span></span>\n\t</a>",
      "source": "NVIDIA Blog (AI)",
      "feed_tags": [
        "infra",
        "chips"
      ],
      "score": 18,
      "primary": "models",
      "tags": [
        "models"
      ],
      "_rid": 14,
      "url": "https://blogs.nvidia.com/blog/ai-sign-language/",
      "entities": [],
      "why": "American Sign Language is the third most prevalent language in the United States — but there are vastly fewer AI tools developed with ASL data than data represe"
    },
    {
      "title": "Explaining AI Without Code: A User Study on Explainable AI",
      "link": "https://arxiv.org/abs/2602.11159",
      "published": "2026-02-13T05:00:00+00:00",
      "summary": "arXiv:2602.11159v1 Announce Type: new \nAbstract: The increasing use of Machine Learning (ML) in sensitive domains such as healthcare, finance, and public policy has raised concerns about the transparency of automated decisions. Explainable AI (XAI) addresses this by clarifying how models generate predictions, yet most methods demand technical expertise, limiting their value for novices. This gap is especially critical in no-code ML platforms, which seek to democratize AI but rarely include explainability. We present a human-centered XAI module in DashAI, an open-source no-code ML platform. The module integrates three complementary techniques, which are Partial Dependence Plots (PDP), Permutation Feature Importance (PFI), and KernelSHAP, into DashAI's workflow for tabular classification. A user study (N = 20; ML novices and experts) evaluated usability and the impact of explanations. Results show: (i) high task success ($\\geq80\\%$) across all explainability tasks; (ii) novices rated explanations as useful, accurate, and trustworthy on the Explanation Satisfaction Scale (ESS, Cronbach's $\\alpha$ = 0.74, a measure of internal consistency), while experts were more critical of sufficiency and completeness; and (iii) explanations improved perceived predictability and confidence on the Trust in Automation scale (TiA, $\\alpha$ = 0.60), with novices showing higher trust than experts. These findings highlight a central challenge for XAI in no-code ML, making explanations both accessible to novices and sufficiently detailed for experts.",
      "source": "arXiv cs.AI",
      "feed_tags": [
        "papers",
        "models"
      ],
      "score": 17,
      "primary": "models",
      "tags": [
        "models"
      ],
      "_rid": 15,
      "url": "https://arxiv.org/abs/2602.11159",
      "entities": [],
      "why": "arXiv:2602.11159v1 Announce Type: new \nAbstract: The increasing use of Machine Learning (ML) in sensitive domains such as healthcare, finance, and public policy"
    }
  ]
}